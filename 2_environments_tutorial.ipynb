{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "domestic-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "generous-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyEnvironment(object):\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"Return initial_time_step.\"\"\"\n",
    "    self._current_time_step = self._reset()\n",
    "    return self._current_time_step\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"Apply action and return new time_step.\"\"\"\n",
    "    if self._current_time_step is None:\n",
    "        return self.reset()\n",
    "    self._current_time_step = self._step(action)\n",
    "    return self._current_time_step\n",
    "\n",
    "  def current_time_step(self):\n",
    "    return self._current_time_step\n",
    "\n",
    "  def time_step_spec(self):\n",
    "    \"\"\"Return time_step_spec.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def observation_spec(self):\n",
    "    \"\"\"Return observation_spec.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def action_spec(self):\n",
    "    \"\"\"Return action_spec.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _reset(self):\n",
    "    \"\"\"Return initial_time_step.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _step(self, action):\n",
    "    \"\"\"Apply action and return new time_step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "posted-antenna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n",
      "time_step_spec.observation: BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
      "time_step_spec.step_type: ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\n",
      "time_step_spec.discount: BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\n",
      "time_step_spec.reward: ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "source": [
    "environment = suite_gym.load('CartPole-v0')\n",
    "print('action_spec:', environment.action_spec())\n",
    "print('time_step_spec.observation:', environment.time_step_spec().observation)\n",
    "print('time_step_spec.step_type:', environment.time_step_spec().step_type)\n",
    "print('time_step_spec.discount:', environment.time_step_spec().discount)\n",
    "print('time_step_spec.reward:', environment.time_step_spec().reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "advanced-world",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.02136502,  0.01631076,  0.04721795,  0.03403996], dtype=float32),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(0)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.0210388 ,  0.2107249 ,  0.04789874, -0.24337931], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.01682431,  0.40513113,  0.04303116, -0.5205774 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.00872168,  0.5996217 ,  0.03261961, -0.799396  ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.00327075,  0.7942814 ,  0.01663169, -1.0816417 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.01915638,  0.9891799 , -0.00500114, -1.3690594 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.03893998,  1.1843641 , -0.03238233, -1.6633024 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.06262726,  1.3798478 , -0.06564838, -1.9658929 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.09022421,  1.5755994 , -0.10496624, -2.2781749 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.12173621,  1.7715279 , -0.15052973, -2.6012533 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.15716676,  1.9674639 , -0.20255479, -2.9359224 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1)})\n",
      "TimeStep(\n",
      "{'discount': array(0., dtype=float32),\n",
      " 'observation': array([ 0.19651604,  2.1631408 , -0.26127324, -3.2825818 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(2)})\n"
     ]
    }
   ],
   "source": [
    "action = np.array(1, dtype=np.int32)\n",
    "time_step = environment.reset()\n",
    "print(time_step)\n",
    "while not time_step.is_last():\n",
    "  time_step = environment.step(action)\n",
    "  print(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "clinical-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardGameEnv(py_environment.PyEnvironment):\n",
    "\n",
    "  def __init__(self):\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
    "    self._state = 0\n",
    "    self._episode_ended = False\n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    self._state = 0\n",
    "    self._episode_ended = False\n",
    "    return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "  def _step(self, action):\n",
    "\n",
    "    if self._episode_ended:\n",
    "      # The last action ended the episode. Ignore the current action and start\n",
    "      # a new episode.\n",
    "      return self.reset()\n",
    "\n",
    "    # Make sure episodes don't go on forever.\n",
    "    if action == 1:\n",
    "      self._episode_ended = True\n",
    "    elif action == 0:\n",
    "      new_card = np.random.randint(1, 11)\n",
    "      self._state += new_card\n",
    "    else:\n",
    "      raise ValueError('`action` should be 0 or 1.')\n",
    "\n",
    "    if self._episode_ended or self._state >= 21:\n",
    "      reward = self._state - 21 if self._state <= 21 else -21\n",
    "      return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "    else:\n",
    "      return ts.transition(\n",
    "          np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "solar-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "environment = CardGameEnv()\n",
    "utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "entire-printing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([0]),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(0)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([5]),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(1)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([15]),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(1)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([16]),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(1)})\n",
      "TimeStep(\n",
      "{'discount': array(0., dtype=float32),\n",
      " 'observation': array([16]),\n",
      " 'reward': array(-5., dtype=float32),\n",
      " 'step_type': array(2)})\n",
      "Final Reward =  -5.0\n"
     ]
    }
   ],
   "source": [
    "get_new_card_action = np.array(0, dtype=np.int32)\n",
    "end_round_action = np.array(1, dtype=np.int32)\n",
    "\n",
    "environment = CardGameEnv()\n",
    "time_step = environment.reset()\n",
    "print(time_step)\n",
    "cumulative_reward = time_step.reward\n",
    "\n",
    "for _ in range(3):\n",
    "  time_step = environment.step(get_new_card_action)\n",
    "  print(time_step)\n",
    "  cumulative_reward += time_step.reward\n",
    "\n",
    "time_step = environment.step(end_round_action)\n",
    "print(time_step)\n",
    "cumulative_reward += time_step.reward\n",
    "print('Final Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "parallel-accused",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Spec: BoundedArraySpec(shape=(1,), dtype=dtype('float32'), name='action', minimum=-2.0, maximum=2.0)\n",
      "Discretized Action Spec: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = suite_gym.load('Pendulum-v0')\n",
    "print('Action Spec:', env.action_spec())\n",
    "\n",
    "discrete_action_env = wrappers.ActionDiscretizeWrapper(env, num_actions=5)\n",
    "print('Discretized Action Spec:', discrete_action_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unable-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFEnvironment(object):\n",
    "\n",
    "  def time_step_spec(self):\n",
    "    \"\"\"Describes the `TimeStep` tensors returned by `step()`.\"\"\"\n",
    "\n",
    "  def observation_spec(self):\n",
    "    \"\"\"Defines the `TensorSpec` of observations provided by the environment.\"\"\"\n",
    "\n",
    "  def action_spec(self):\n",
    "    \"\"\"Describes the TensorSpecs of the action expected by `step(action)`.\"\"\"\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"Returns the current `TimeStep` after resetting the Environment.\"\"\"\n",
    "    return self._reset()\n",
    "\n",
    "  def current_time_step(self):\n",
    "    \"\"\"Returns the current `TimeStep`.\"\"\"\n",
    "    return self._current_time_step()\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"Applies the action and returns the new `TimeStep`.\"\"\"\n",
    "    return self._step(action)\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _reset(self):\n",
    "    \"\"\"Returns the current `TimeStep` after resetting the Environment.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _current_time_step(self):\n",
    "    \"\"\"Returns the current `TimeStep`.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _step(self, action):\n",
    "    \"\"\"Applies the action and returns the new `TimeStep`.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "protected-paste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "TimeStep Specs: TimeStep(\n",
      "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32)),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
      "Action Specs: BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0, dtype=int64), maximum=array(1, dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "print(isinstance(tf_env, tf_environment.TFEnvironment))\n",
    "print(\"TimeStep Specs:\", tf_env.time_step_spec())\n",
    "print(\"Action Specs:\", tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "public-pressing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TimeStep(\n",
      "{'discount': array([1.], dtype=float32),\n",
      " 'observation': array([[ 0.00688582, -0.02496718,  0.02946392,  0.02648936]],\n",
      "      dtype=float32),\n",
      " 'reward': array([0.], dtype=float32),\n",
      " 'step_type': array([0])}), array([0]), TimeStep(\n",
      "{'discount': array([1.], dtype=float32),\n",
      " 'observation': array([[ 0.00638647, -0.22049901,  0.02999371,  0.32832092]],\n",
      "      dtype=float32),\n",
      " 'reward': array([1.], dtype=float32),\n",
      " 'step_type': array([1])})]\n",
      "[TimeStep(\n",
      "{'discount': array([1.], dtype=float32),\n",
      " 'observation': array([[ 0.00638647, -0.22049901,  0.02999371,  0.32832092]],\n",
      "      dtype=float32),\n",
      " 'reward': array([1.], dtype=float32),\n",
      " 'step_type': array([1])}), array([1]), TimeStep(\n",
      "{'discount': array([1.], dtype=float32),\n",
      " 'observation': array([[ 0.00197649, -0.02581661,  0.03656013,  0.04524551]],\n",
      "      dtype=float32),\n",
      " 'reward': array([1.], dtype=float32),\n",
      " 'step_type': array([1])})]\n",
      "[TimeStep(\n",
      "{'discount': array([1.], dtype=float32),\n",
      " 'observation': array([[ 0.00197649, -0.02581661,  0.03656013,  0.04524551]],\n",
      "      dtype=float32),\n",
      " 'reward': array([1.], dtype=float32),\n",
      " 'step_type': array([1])}), array([0]), TimeStep(\n",
      "{'discount': array([1.], dtype=float32),\n",
      " 'observation': array([[ 0.00146016, -0.22144322,  0.03746504,  0.34923562]],\n",
      "      dtype=float32),\n",
      " 'reward': array([1.], dtype=float32),\n",
      " 'step_type': array([1])})]\n",
      "Total reward: [3.]\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "# reset() creates the initial time_step after resetting the environment.\n",
    "time_step = tf_env.reset()\n",
    "num_steps = 3\n",
    "transitions = []\n",
    "reward = 0\n",
    "for i in range(num_steps):\n",
    "  action = tf.constant([i % 2])\n",
    "  # applies the action and returns the new TimeStep.\n",
    "  next_time_step = tf_env.step(action)\n",
    "  transitions.append([time_step, action, next_time_step])\n",
    "  reward += next_time_step.reward\n",
    "  time_step = next_time_step\n",
    "\n",
    "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
    "print('\\n'.join(map(str, np_transitions)))\n",
    "print('Total reward:', reward.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "identical-oriental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_episodes: 5 num_steps: 107\n",
      "avg_length 21.4 avg_reward: 21.4\n"
     ]
    }
   ],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "  episode_reward = 0\n",
    "  episode_steps = 0\n",
    "  while not time_step.is_last():\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)\n",
    "\n",
    "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
    "print('avg_length', avg_length, 'avg_reward:', avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-specialist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
